{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                \n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bv_id(url):\n",
    "    match = re.search(r'video/(BV\\w+)', url)\n",
    "    if match:\n",
    "        return match.group(1)  \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bulletcomment(link):\n",
    "    '''\n",
    "    given url, get the bullet comment of that video\n",
    "    '''\n",
    "    BV = extract_bv_id(link)\n",
    "    #BV = re.search(r'/video/(BV\\w+)/', link).group(1) # for each video it has an unique BV id in its url\n",
    "    apilink = 'https://api.bilibili.com/x/player/pagelist?bvid=' + BV + '&jsonp=jsonp'\n",
    "\n",
    "    header = { \"User-Agent\" : \"demo scraper for teaching purposes ys@u.edu\" } \n",
    "    response = requests.get(apilink, headers = header)\n",
    "    cid = (response.json())['data'][0]['cid']\n",
    "\n",
    "    cidlink = 'https://comment.bilibili.com/'+ str(cid) + '.xml'\n",
    "    response = requests.get(cidlink, headers = header)\n",
    "    soup = BeautifulSoup(response.text, \"lxml-xml\")\n",
    "    texts = [d.text for d in soup.find_all('d')]\n",
    "    return texts\n",
    "\n",
    "\n",
    "\n",
    "#data = get_bulletcomment('https://www.bilibili.com/video/BV1wD421W7GP/?spm_id_from=333.1007.tianma.1-3-3.click&vd_source=a676e9574ae10f45ae8a73f5c6c428fd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bulletcomment_score(link):\n",
    "    '''\n",
    "    given url, get the bullet comment of that video\n",
    "    '''\n",
    "    BV = extract_bv_id(link)\n",
    "    #BV = re.search(r'/video/(BV\\w+)/', link).group(1) # for each video it has an unique BV id in its url\n",
    "    apilink = 'https://api.bilibili.com/x/player/pagelist?bvid=' + BV + '&jsonp=jsonp'\n",
    "\n",
    "    header = { \"User-Agent\" : \"demo scraper for teaching purposes ys@u.edu\" } \n",
    "    response = requests.get(apilink, headers = header)\n",
    "    cid = (response.json())['data'][0]['cid']\n",
    "\n",
    "    cidlink = 'https://comment.bilibili.com/'+ str(cid) + '.xml'\n",
    "    response = requests.get(cidlink, headers = header)\n",
    "    soup = BeautifulSoup(response.text, \"lxml-xml\")\n",
    "    #texts = [d.text.encode('latin1').decode('utf-8') for d in soup.find_all('d')]\n",
    "\n",
    "    d_tags = soup.find_all('d')\n",
    "    output = []\n",
    "    for tag in d_tags:\n",
    "    # Split the 'p' attribute by commas and get the last element\n",
    "        score = tag['p'].split(',')[-1]\n",
    "        text = tag.text.encode('latin1').decode('utf-8')\n",
    "        output.append([text, int(score)])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through a list of video to find porn seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_video(link):\n",
    "    '''\n",
    "    given url, find a list of related short video\n",
    "    '''\n",
    "    header = { \"User-Agent\" : \"demo scraper for teaching purposes ys@u.edu\" } \n",
    "    response = requests.get(link, headers = header)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    videos = soup.find_all('div', class_=\"video-page-card-small\")\n",
    "    video_links = []\n",
    "    for video in videos:\n",
    "        a_tag = video.find('a')\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            video_links.append('https://www.bilibili.com' + a_tag['href'])\n",
    "    return video_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def get_n_related_video(link, n):\n",
    "    '''\n",
    "    given url, find a list of short video with size n \n",
    "    '''\n",
    "    video_links = [link]\n",
    "    BV = []\n",
    "    current_link = link\n",
    "    while len(video_links) < n:\n",
    "        print(len(video_links))\n",
    "        print(current_link)\n",
    "        related_link = get_related_video(current_link)\n",
    "        #for i in related_link:\n",
    "        #    if extract_bv_id(i) not in BV:\n",
    "        #        BV.append(extract_bv_id(i))\n",
    "        #        video_links.append(i)\n",
    "        #current_link = random.choice(related_link)\n",
    "\n",
    "        if extract_bv_id(related_link[0]) not in BV:\n",
    "            BV.append(extract_bv_id(related_link[0]))\n",
    "            video_links.append(related_link[0])\n",
    "            current_link = related_link[0]\n",
    "        elif extract_bv_id(related_link[1]) not in BV:\n",
    "            BV.append(extract_bv_id(related_link[1]))\n",
    "            video_links.append(related_link[1])\n",
    "            current_link = related_link[1]\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    \n",
    "\n",
    "    return video_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = get_n_related_video('https://www.bilibili.com/video/BV1yFUJYsE5h/?spm_id_from=333.880.my_history.page.click&vd_source=a676e9574ae10f45ae8a73f5c6c428fd',300)\n",
    "print(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_porn(links):\n",
    "    '''\n",
    "    check whether the videos inside a list contains the RIP as a bullet chat\n",
    "    '''\n",
    "    potential_porn = []\n",
    "    j=0\n",
    "    for link in links:\n",
    "        print([j, len(potential_porn)])\n",
    "        j+=1\n",
    "        time.sleep(1)\n",
    "        for i in get_bulletcomment(link):\n",
    "            if i.encode('latin1').decode('utf-8') == '逝者安息':\n",
    "                potential_porn.append(link)\n",
    "    return(potential_porn)\n",
    "        \n",
    "potential_porn = check_porn(videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key word searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_keyword_links(url):\n",
    "    '''\n",
    "    given a url (from key word searching), provide the links for all the video under this key word\n",
    "    '''\n",
    "    header = { \"User-Agent\" : \"demo scraper for teaching purposes ys@u.edu\" } \n",
    "    response = requests.get(url, headers = header)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    videos = soup.find_all('div', class_=\"bili-video-card__info--right\")\n",
    "    video_links = []\n",
    "    for video in videos:\n",
    "        a_tag = video.find('a')\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            video_links.append('https:' + a_tag['href'])\n",
    "    return video_links\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "HK_riot = fetch_keyword_links(\"https://search.bilibili.com/all?keyword=%E9%A6%99%E6%B8%AF%E6%9A%B4%E4%B9%B1&from_source=webtop_search&spm_id_from=333.32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(HK_riot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.bilibili.com/video/BV1cJ411274r/'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HK_riot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_auth(url):\n",
    "    '''\n",
    "    given a video, check whether its uploader is an certified user\n",
    "    '''\n",
    "    try: \n",
    "        header = { \"User-Agent\" : \"demo scraper for teaching purposes ys@u.edu\" } \n",
    "        response = requests.get(url, headers = header)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        user_info = soup.find('div', class_=\"up-detail-top\")\n",
    "        user_name = user_info.find('a').text\n",
    "        user_type = user_info.find('a')['class']\n",
    "        user_color = user_info.find('a')['style']\n",
    "        if user_color == \"color:#FB7299;\":\n",
    "            return \"government\"\n",
    "        return \"non_government\"\n",
    "    except:\n",
    "        return \"wrong\"\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'government'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_auth(HK_riot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the HK_riot list, get all the comment, score, and user_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "HK_comment = []\n",
    "for i in HK_riot:\n",
    "    User = check_auth(i)\n",
    "    if User != \"wrong\":\n",
    "        HK_comment.append([User, get_bulletcomment_score(i)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HK_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36534"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=0\n",
    "for i in HK_comment:\n",
    "    n += len(i[1])\n",
    "n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "b_values = [\n",
    "    sub_item[0]  # 提取 B\n",
    "    for item in HK_comment if isinstance(item, list) and len(item) > 1\n",
    "    for sub_item in item[1] if isinstance(sub_item, list) and len(sub_item) > 0\n",
    "]\n",
    "\n",
    "# 统计每个 B 的出现次数\n",
    "b_counts = Counter(b_values)\n",
    "\n",
    "# 找出前十个最常见的 B\n",
    "top_200_b_values = b_counts.most_common(200)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Top 10 most common B values:\")\n",
    "for b, count in top_200_b_values:\n",
    "    print(f\"{b}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST: 脏话，卧槽， 恶心， 人渣， 垃圾， 草， 丧心病狂， 狗， 枪毙， 汉奸， 杀， 神经病， 死"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average highest value for '心疼' in government: 9.55\n",
      "Average highest value for '心疼' in non_government: 8.50\n"
     ]
    }
   ],
   "source": [
    "# 指定的 B*（允许部分匹配）\n",
    "target_b = \"心疼\"\n",
    "\n",
    "# 初始化存储最高值的字典\n",
    "results = {\"government\": [], \"non_government\": []}\n",
    "\n",
    "# 计算每个元素中包含 target_b 的最高 C 值并分类\n",
    "for item in HK_comment:\n",
    "    # 检查是否符合 [A, [[B, C], ...]] 的结构\n",
    "    if isinstance(item, list) and len(item) == 2 and isinstance(item[1], list):\n",
    "        # 提取 A 的值\n",
    "        government_status = item[0]\n",
    "        \n",
    "        # 查找包含 target_b 的最高 C 值\n",
    "        max_c = max(\n",
    "            [sub_item[1] for sub_item in item[1] if target_b in sub_item[0]], \n",
    "            default=None  # 如果没有匹配，返回 None\n",
    "        )\n",
    "        \n",
    "        # 如果找到了 C 值，添加到对应类型中\n",
    "        if max_c is not None:\n",
    "            results[government_status].append(max_c)\n",
    "\n",
    "# 计算平均值\n",
    "average_government = sum(results[\"government\"]) / len(results[\"government\"]) if results[\"government\"] else 0\n",
    "average_non_government = sum(results[\"non_government\"]) / len(results[\"non_government\"]) if results[\"non_government\"] else 0\n",
    "\n",
    "#average_government = sum(results[\"government\"]) / 17 if results[\"government\"] else 0\n",
    "#average_non_government = sum(results[\"non_government\"]) / 24 if results[\"non_government\"] else 0\n",
    "\n",
    "# 打印结果\n",
    "print(f\"Average highest value for '{target_b}' in government: {average_government:.2f}\")\n",
    "print(f\"Average highest value for '{target_b}' in non_government: {average_non_government:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 24\n"
     ]
    }
   ],
   "source": [
    "G = 0\n",
    "N = 0\n",
    "for i in HK_comment:\n",
    "    if i[0] == \"government\":\n",
    "        G+=1\n",
    "    if i[0] == \"non_government\":\n",
    "        N+=1\n",
    "print (G,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weekly Top word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weekly_video_links(url):\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        video_links = []\n",
    "        \n",
    "        videos = soup.find_all('div', class_=\"video-card__content\")\n",
    "        for video in videos:\n",
    "            a_tag = video.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                video_links.append(a_tag['href'])\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return video_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bilibili.com/v/popular/weekly?num=267\"\n",
    "links = fetch_video_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "写入文件成功\n"
     ]
    }
   ],
   "source": [
    "data_processed = []\n",
    "for i in links:\n",
    "    data = get_bulletcomment(i)\n",
    "    for j in data:\n",
    "        j = j.encode('latin1').decode('utf-8')\n",
    "        data_processed.append(j)\n",
    "df = pd.DataFrame(data_processed)\n",
    "df.to_csv('超大弹幕', index=False, header=None, encoding=\"utf_8_sig\")\n",
    "print(\"写入文件成功\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_top_duplicates(filename, top_n=100):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        # 读取所有行，并去除两端的空白字符（如换行符）\n",
    "        lines = [line.strip() for line in file if line.strip()]\n",
    "    \n",
    "    # 使用Counter统计每行出现的次数\n",
    "    line_counts = Counter(lines)\n",
    "    \n",
    "    # 获取出现次数最多的top_n行\n",
    "    top_lines = line_counts.most_common(top_n)\n",
    "    \n",
    "    return top_lines\n",
    "\n",
    "# 调用函数，假设文件名为'超大弹幕.txt'\n",
    "top_duplicates = find_top_duplicates('超大弹幕')\n",
    "for line, count in top_duplicates:\n",
    "    print(f'\"{line}\" 出现了 {count} 次')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##test\n",
    "data_processed = []\n",
    "for i in data:\n",
    "    i = i.encode('latin1').decode('utf-8')\n",
    "    data_processed.append(i)\n",
    "df = pd.DataFrame(data_processed)\n",
    "df.to_csv('弹幕', index=False, header=None, encoding=\"utf_8_sig\")\n",
    "print(\"写入文件成功\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gz = get_bulletcomment('https://www.bilibili.com/video/BV1Ym411S7sq/?spm_id_from=333.337.search-card.all.click&vd_source=a676e9574ae10f45ae8a73f5c6c428fd')\n",
    "#data_processed = []\n",
    "for i in gz:\n",
    "    i = i.encode('latin1').decode('utf-8')\n",
    "    data_processed.append(i)\n",
    "df = pd.DataFrame(data_processed)\n",
    "df.to_csv('贵州', index=False, header=None, encoding=\"utf_8_sig\")\n",
    "print(\"写入文件成功\")\n",
    "top_duplicates = find_top_duplicates('贵州')\n",
    "for line, count in top_duplicates:\n",
    "    print(f'\"{line}\" shows {count} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_censorscore(link):\n",
    "    '''\n",
    "    given url, get the bullet comment of that video\n",
    "    '''\n",
    "    BV = extract_bv_id(link)\n",
    "    #BV = re.search(r'/video/(BV\\w+)/', link).group(1) # for each video it has an unique BV id in its url\n",
    "    apilink = 'https://api.bilibili.com/x/player/pagelist?bvid=' + BV + '&jsonp=jsonp'\n",
    "\n",
    "    header = { \"User-Agent\" : \"demo scraper for teaching purposes ys@u.edu\" } \n",
    "    response = requests.get(apilink, headers = header)\n",
    "    cid = (response.json())['data'][0]['cid']\n",
    "\n",
    "    cidlink = 'https://comment.bilibili.com/'+ str(cid) + '.xml'\n",
    "    response = requests.get(cidlink, headers = header)\n",
    "    soup = BeautifulSoup(response.text, \"lxml-xml\")\n",
    "    d_tags = soup.find_all('d')\n",
    "    scores = []\n",
    "    for tag in d_tags:\n",
    "    # Split the 'p' attribute by commas and get the last element\n",
    "        score = tag['p'].split(',')[-1]\n",
    "        scores.append(int(score))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = get_censorscore('https://www.bilibili.com/video/BV1rCsdeEEBT/?spm_id_from=333.337.search-card.all.click&vd_source=a676e9574ae10f45ae8a73f5c6c428fd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = {x: s.count(x) for x in set(s)}\n",
    "total = len(s)\n",
    "percentages = {k: v / total * 100 for k, v in counts.items()}\n",
    "\n",
    "# X and Y data for the plot\n",
    "x = list(percentages.keys())\n",
    "y = list(percentages.values())\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, y, color='skyblue')\n",
    "plt.title('Guanca.cn Censor Score Distribution')\n",
    "plt.xlabel('Censor Score')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xticks(range(0, max(x) + 1))  # Set x-ticks to start from 0 and go beyond the maximum x value\n",
    "plt.xlim(0, max(x) + 1)  # Set the x-axis limits to start at 0 and extend beyond the max value\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add horizontal grid lines for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Automatically download and use the correct ChromeDriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Initialize the WebDriver with the Service\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Your browser automation code here\n",
    "driver.get(\"https://www.bilibili.com/blackroom/ban\")\n",
    "\n",
    "# 模拟滚动加载\n",
    "SCROLL_PAUSE_TIME = 1\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # 滚动到底部\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(SCROLL_PAUSE_TIME)  # 等待页面加载\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:  # 判断是否还有新内容加载\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# 抓取页面数据\n",
    "content = driver.page_source\n",
    "print(content)\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
